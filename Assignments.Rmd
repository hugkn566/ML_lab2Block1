---
title: "Lab Topic 2 Block 1 Machine Learning"
author: "Hugo Knape & Zahra Jalil Pour & Niklas Larsson"
date: "11/12/2020"
output:
word_document: default
pdf_document: default
latex_engine: xelatex
---
  
# State of contribution 

### Assignment1: Zahra Jalilpour
### Assignment2: Niklas Larsson
### Assignment3: Hugo Knap
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, include = FALSE}
library(kknn)
library(ggplot2)
library(e1071)
library(tree)
library(rpart)
```

# Assignment 1. 
---
title: "Untitled"
author: "z"
date: '2020-11-20'
output:
  html_document: default
  pdf_document: default
  latex_engine: xelatex
---



```{r, warning = FALSE, include = FALSE}
library(ggplot2)
library(datasets)

```
# Assignment 1: LDA and logistic regression


In LDA , features in each class has multivariate normal distribution and common variance $ \Sigma$. The covariance matrix is the same in all class  $Cov(X) = \Sigma$. Random variable $X$ is a vector $X=(X_{1}, X_{2},...,X_{p})$
Mean of each class is :
$$\hat\mu_k = \frac{1}{\#\{i\;;\;y_i=k\}}\sum_{i\;;\; y_i=k} x_i$$
Perior probabity or $ \ pi_k$ is equal to: 
$$ \hat \pi_k = \frac{\{i\;;\;y_i=k\}}{n}$$
$X$ follows multivariate Gaussian distribution.


## Task 1

### Make a scatterplot
```{r}
ggplot(iris, aes(x=Sepal.Width , y= Sepal.Length, shape= Species, color=Species)) + geom_point() +stat_ellipse()

```
Here we can see that we have more than two classes. Then LDA is the preferred linear classification technique, and LDA is a simple model for preparation and application. Here we can not use logistic regression, because we have more than two classes. Although we can use multi logistic regression but is rarely used for this purpose. LDA is a stable model when the classes are separated well. Also LDA generally is used for small data sets. 

## Task 2

### 2-a
Compute mean, covariance matrix and prior probabilities for each class: 


```{r,warning=FALSE, message=FALSE,echo=F }
data(iris)
### covariance matrix per each class
paste("Covariance matrix in Setosa class")
(var(subset(iris,subset=Species=='setosa',select=c(1:2))) -> S1)
paste("Covariance matrix in Versicolor class")
(var(subset(iris,subset=Species=='versicolor',select=c(1:2))) -> S2)
paste("Covariance matrix in Virginica class")
(var(subset(iris,subset=Species=='virginica',select=c(1:2))) -> S3)


### mean per each class

m1 <- colMeans(subset(iris,subset=Species=='setosa',select=c(1:2))) 

m2 <- colMeans(subset(iris,subset=Species=='versicolor',select=c(1:2))) 

m3 <- colMeans(subset(iris,subset=Species=='virginica',select=c(1:2)))

paste("mean for each class")
results <- cbind(m1,m2,m3)
colnames(results) <- c("setosa","versicolor","virginica")
rownames(results) <- c("Sepal.Length", "Sepal.Width")
results


### prior probability per each class
paste("prior probability for each class: ")
(nrow(subset(iris,subset=Species=='setosa')) / nrow(iris) -> prio_prob1)
(nrow(subset(iris,subset=Species=='versicolor')) / nrow(iris) -> prio_prob2)
(nrow(subset(iris,subset=Species=='virginica')) / nrow(iris) -> prio_prob3)

```
### 2-b
pooled covariance matrix:
```{r,warning=FALSE, message=FALSE,echo=F}
paste("degree of freedom: ")
(table(iris$Species)-1 -> dof)


paste("pooled covariance matrix: ")
((dof[1]*S1+dof[2]*S2+dof[3]*S3)/sum(dof) -> S)
```
### 2-c
probabilistic model: 
The probability of $p(X = x | Y = k)$ is given by:
$$f_k(x) = \frac{1}{(2 \pi)^{p/2} |\Sigma|^{1/2}} \
exp \left( - \frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)$$
By considering prior probability is $P(Y = k) = \pi_k$ , and taking logarithm , we will find Linear discriminant function or Linear score function:



$$\delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k} + log(\pi_k)$$
$$\hat{G}(x)= \text{ arg }\underset{k}{max}\delta_k(x)$$

The decision boundary is the set of points in which two classes are equally probable:
$$\delta_k(x) = \delta_l(x)$$



We consider $ \pi_k, \ \mu_k, \ \Sigma$ by MLE as below:
$$\hat \Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T$$
$$ \hat \pi_k = \frac{\{i\;;\;y_i=k\}}{n}$$
```{r,warning=FALSE, message=FALSE,echo=F}
multi_gusi_model <- function(x,mean,sigma){
  p <- ncol(x)
  sig_inv <- solve(sigma)
  state1 <- (sqrt((2*pi)^p) *(det(sigma)))
  state2 <- -(1/2)*(as.matrix(x-mean) %*% solve(sigma) %*% t(x-mean) )
  model <- (1/state1) * exp(state2)
  return(model)
}

```


### 2_d
Compute discriminant functions for each class:
```{r,warning=FALSE, message=FALSE,echo=F}
discriminant <- function(x,mean_i, pooled_cov, prior_i){
  #mu <- as.matrix(mean_i)
  #browser()
  state1 <- as.matrix(x) %*% solve(pooled_cov) %*% mean_i
  state2 <- 0.5 * t(mean_i) %*% solve(pooled_cov) %*% mean_i
  d <- state1 - as.numeric(state2) + as.numeric(log(prior_i))
  return(d)
}
#example for one class
#pooled <- ((dof[1]*S1+dof[2]*S2+dof[3]*S3)/sum(dof) -> S)
prior <- (nrow(subset(iris,subset=Species=='setosa')) / nrow(iris) -> prio_prob1)
d1 <-discriminant(iris[c(1,2)] ,matrix(m1,ncol = 1),S,prio_prob1)

d2 <-discriminant(iris[c(1,2)] ,matrix(m2,ncol = 1),S,prio_prob2)

d3 <-discriminant(iris[c(1,2)] ,matrix(m3,ncol = 1),S,prio_prob3)

D_mat <- cbind(d1,d2,d3)
colnames(D_mat) <- c("setosa","versicolor","virginica")
paste("Discriminant matrix for three different species" )
D_mat
```
### 2-e
Compute equation of decision boundaries between classes:

```{r,warning=FALSE, message=FALSE,echo=F}
w0 <- function(pi_k, pi_l, mu_k, mu_l, pooled_cov){
  m1 <- as.matrix(m1)
  m2 <- as.matrix(m2)
  w0 <- -(1/2)*t(mu_k + mu_l) %*% solve(pooled_cov) %*% (mu_k - mu_l)
  return(w0)
}
w1 <-function(pi_k, pi_l, mu_k, mu_l, pooled_cov){
  m1 <- as.matrix(m1)
  m2 <- as.matrix(m2)
  w1 <-  solve(pooled_cov) %*% (mu_k - mu_l)
  return(as.matrix(w1))
} 
msg1 <- paste("decision boundary for Setosa ~ Versicolor\n\n")
msg2 <- paste(msg1, "W1 : ")
noquote(strsplit(msg2, "\n")[[1]])
w1(0.33,0.33,m1,m2,S)
paste("W0 : ", w0(0.33,0.33,m1,m2,S))

msg1 <- paste("decision boundary for Versicolor ~ Verginica\n\n")
msg2 <- paste(msg1, "W1 : ")
noquote(strsplit(msg2, "\n")[[1]])
w1(0.33,0.33,m2,m3,S)
paste("W0 : ",w0(0.33,0.33,m2,m3,S))

msg1 <- paste("decision boundary for Verginica ~ Setosa\n\n")
msg2 <- paste(msg1, "W1 : ")
noquote(strsplit(msg2, "\n")[[1]])
w1(0.33,0.33,m3,m1,S)
paste("W0 :  ", w0(0.33,0.33,m3,m1,S))
```

### 3-a
predict function:
```{r}
library(ggplot2)
D <- cbind(d1,d2,d3)

for(i in row(D)){
  predict <-apply(X = D[,], MARGIN = 1, FUN = which.max)
}
new_df <- iris
new_df <- cbind(new_df, predict)
new_df$predict[new_df$predict == "1"] <- "setosa"
new_df$predict[new_df$predict == "2"] <- "versicolor"
new_df$predict[new_df$predict == "3"] <- "virginica"
ggplot(new_df, aes(x=Sepal.Length , y= Sepal.Width, shape= predict, color=predict)) + geom_point()+stat_ellipse()

Actual<- iris$Species
t_3 <-table(Actual, new_df$predict)
knitr::kable(t_3, caption = "MissClassification matrix for manual prediction")
lda_error_3 = 1- sum(diag(t_3))/sum(t_3)
paste("MSE for manual LDA: " ,lda_error_3)
```


### 3_b  
lda function
```{r}
library(MASS)
new_data3c <- iris
fitted_lda <- lda(Species~Sepal.Length + Sepal.Width, data = new_data3c)
fitted_lda
#### Confusion Matrix###
Classification <- predict(fitted_lda, data = new_data3c)$class
Actual<- iris$Species
t <-table(Actual, Classification)
knitr::kable(t,caption = "MissClassification matrix for LDA")
lda_error = 1- sum(diag(t))/sum(t)
paste("MSE of LDA model: ", lda_error)
```
```{r}

```

Miss classification matrix and Accuracy in both model are same.
```{r,warning=FALSE}
library("mvtnorm")
bvn1 <- as.data.frame(mvrnorm(50, mu =m1, S ))
bvn2 <- as.data.frame(mvrnorm(50, mu=m2, S))
bvn3 <- as.data.frame(mvrnorm(50, mu=m3, S))
bv <- rbind(bvn1,bvn2,bvn3)

ggplot() +
  geom_point(data=bv, aes(bv$Sepal.Length, bv$Sepal.Width, color=iris$Species))
```
By using new generate data, we can see the Setosa species is classified better than two classes, like other models.
### 5 
Logistic Regression
```{r,warning=FALSE}
library(nnet)
multi_model <- multinom(Species~Sepal.Length + Sepal.Width, data =iris)
multi_model
new_data <- iris
new_data$predicted<- predict(multi_model, new_data, type="class")
summary(new_data)
t_multi <- table(new_data$Species, new_data$predicted)
t_multi
ME_error <- 1-sum(diag(t_multi))/sum(t_multi)
paste( "ME_error= ", ME_error)


ggplot(new_data, aes(x=Sepal.Width , y= Sepal.Length, shape= predicted, color=predicted)) + geom_point() +stat_ellipse()

```
We can see here that ME in logistic regression model is lower than two other models.


# Assignment 2. Decision trees and NaÃ¯ve Bayes for bank marketing

## Task 1
Pre-process data by removing "Duration" feature, converting all character columns to categorical classes (factors) and split into train-, validation- and testset.
```{r}

data = read.csv2("bank-full.csv")
data$duration = NULL

character_vars = lapply(data, class) == "character"
data[, character_vars] = lapply(data[, character_vars], as.factor)
#str(data)


n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.4))
train=data[id,]

id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.3))
valid=data[id2,]

id3=setdiff(id1,id2)
test=data[id3,]

#sum(c(dim(train)[1],dim(valid)[1], dim(test)[1])) == dim(data)[1]

```

## Task 2
Fit three different models from the given settings.
```{r}
n = dim(train)[1]
fit_def = tree(formula = y ~., data=train, control = tree.control(nobs = n))
fit_node = tree(formula = y ~., data=train, control = tree.control(nobs = n, minsize = 7000))
fit_dev = tree(formula = y ~., data=train, control = tree.control(nobs = n, mindev = 0.0005))

misclass = matrix(0,3,2)
dimnames(misclass) = list( c("Default","Node","Deviance"), c("Test","Validation") )


pred_def_train = predict(fit_def, newdata = train, type = "class")
pred_def_val = predict(fit_def, newdata = valid, type = "class")
misclass[1,1] = sum(train$y != pred_def_train)/length(train$y)
misclass[1,2] = sum(valid$y != pred_def_val)/length(valid$y)


pred_node_train = predict(fit_node, newdata = train, type = "class")
pred_node_val = predict(fit_node, newdata = valid, type = "class")
misclass[2,1] = sum(train$y != pred_node_train)/length(train$y)
misclass[2,2] = sum(valid$y != pred_node_val)/length(valid$y)


pred_dev_train = predict(fit_dev, newdata = train, type = "class")
pred_dev_val = predict(fit_dev, newdata = valid, type = "class")
misclass[3,1] = sum(train$y != pred_dev_train)/length(train$y)
misclass[3,2] = sum(valid$y != pred_dev_val)/length(valid$y)

misclass

```
From this result the default/nodesize trees performs best on validation data while deviance model does the opposite.
Judging by these result the Deviance-model would be overfitting compared to the other two which also could be seen in the graphical illustrations below.
The deviance model do have more potential to be optimized as this tree is much larger. The default and nodesize does only differ by one node which can explain why they perform identically!

Increasing the minimum node size results in a smaller tree as the data is divided in to larger areas (each area represent a terminal node) and there are less "classification squares" to decide a label between.

Decreasing the deviance forces the model to split nodes more often which will result in a larger tree as seen below.
This does introduce more risk for overfitting. The deviance is calculated by $$Deviance = -\sum_{i=1}^n p(c_i)log(p(c_i))$$ where $p(c_i)$ is the probability of class $c_i$ in the node. Using a lower value for deviance forces the probability for classification to be higher which results in more splits.

```{r}
plot(fit_def, type = "uniform")
plot(fit_node, type = "uniform")
plot(fit_dev, type = "uniform")
```
## Task 3

```{r}

trainScore = rep(0,50)
testScore = rep(0,50)

for(i in 2:50){
  prunedTree = prune.tree(fit_dev, best = i)
  pred = predict(prunedTree, newdata = valid, type = "tree" )
  trainScore[i] = deviance(prunedTree)
  testScore[i] = deviance(pred)
}
```

As seen in the figure below the trees total deviance decreases rapidly during the first part as the model is able to distinguish between more features. As the number of leaves increases the training data's deviance keeps decreasing while the validation data starts to increase after around 20 leafs. This is where the optimal tree size is found.
The reason why the training-deviance is larger than the validations is due to the training set being larger therefore more values to sum up.

```{r}

plot(2:50, trainScore[2:50], type = "b", col = "red", ylim = c(8000,12000), main = "Task 3", xlab = "Sequence", ylab = "Deviance")
points(2:50, testScore[2:50], type = "b", col = "blue")
legend("topright", c("Train","Validation"), pch = c("o","o"), col= c("red","blue"))

```
The optimal size (number of leaves/terminal nodes) is found where at the minimum deviance.
The optimal number of leaves and the most significant features is shown in the summary of the optimal model:

```{r}
opt_leaf = which.min(testScore[2:50])
opt_mdl = prune.tree(fit_dev, best = opt_leaf)
summary(opt_mdl)
```

The tree structure seems to use more terminal nodes than necessary as there are a lot of nodes which leads to the same label/ typ of terminal node, could be due to the deviance setting. The number of terminal nodes are indeed the same as the optimal which where found above.
The outcome feature seems to be the most significant as it is set as the root node. This feature indicate wherever the customer was persuaded to subscribe a term deposit, which seems like a good starting point.

```{r}
plot(opt_mdl, type = "uniform")
text(opt_mdl)
```
The prediction power of this model is ok, it does get the right answer in about 9/10 cases but its' main failure is by classifying False Negatives.

```{r}
pred_valid = predict(opt_mdl, newdata = valid, type = "class")
val_error = sum(pred_valid != valid$y)/length(valid$y)
val_conf_matrix = table(valid$y, pred_valid)
val_conf_matrix
sprintf("Misclassification rate: %.4f", val_error)
```



## Task 4
By introducing a loss matrix to penalize the False Negative predictions the misclassification rate did increase but the number of False Negative classifications decreased!
Using a loss matrix will direct the focus of learning towards the weighting of the matrix, in this case classifying False Negatives gives a five time larger penalty than False Positive and this will affect the overall classification which is why error rate increased.

Using the loss matrix does give a good result as the previous model have been trained on both the training and validation set while this model only have been exposed to the training set, suggesting that it learn faster (but more knowledge about the data is required to set up the loss matrix).


```{r}


lossMatrix = matrix(c(0,5,1,0),2,2)

fit_matrix = rpart(formula = y~., data = train, parms = list(loss = lossMatrix))
pred_matrix = predict(fit_matrix, newdata = test, type = "class")
error = sum(pred_matrix != test$y)/length(test$y)
cm_matrix = table(test$y, pred_matrix)
cm_matrix
sprintf("Misclassification rate: %.4f", error)
```
 
## Task 5


As can be seen in the graph below the optimal tree model seems to be the best model as the "area under curve" (AUC) is greater than the naive's model. Comparing two models AUC could occasionally lead the wrong assumption within certain regions but in practice the AUC-measure performs well as a general comparison. The AUC is calculated using the trapezoidal rule.
In this case the tree models ROC curve is always above the naive's which also indicates that the tree model would perform best.
Both models are well above the dashed line which could be interpret as a minimum boarder for a models predictive power, if a models is below this line the predictive power is worse than random guessing.

```{r}


fit_naive = naiveBayes(formula = y~., data = train)

opt_pred = predict(opt_mdl, newdata = test, type = "vector")
naive_pred = predict(fit_naive, newdata = test, type = "raw")

thres = seq(0, 0.95, 0.05)
TPR_tree = rep(0,length(thres))
FPR_tree = rep(0,length(thres))
TPR_naive = rep(0,length(thres))
FPR_naive = rep(0,length(thres))
y = test$y


for(i in 1:length(thres)){
  TP_tree = 0
  FP_tree = 0
  TP_naive = 0
  FP_naive = 0
  
  for(j in 1:length(test$y)){
      
    if(opt_pred[j,2] >= thres[i]){ 
      if(y[j] == "yes"){ 
        TP_tree = TP_tree + 1
        }
      else{
        FP_tree = FP_tree + 1
        }
      }
    
    if(naive_pred[j,2] >= thres[i]){ 
      if(y[j] == "yes"){ 
        TP_naive = TP_naive + 1 
        }
      else{ 
        FP_naive = FP_naive + 1 
        }
    }
      
  }
  TPR_tree[i] = TP_tree/sum(test$y == "yes")
  FPR_tree[i] = FP_tree/sum(test$y == "no")
  
  TPR_naive[i] = TP_naive/sum(test$y == "yes")
  FPR_naive[i] = FP_naive/sum(test$y == "no")
}

```

```{r, ECHO = FALSE}

sub_model = seq(0,1,0.01)
AUC_tree = sum(abs(diff(FPR_tree)) * (head(TPR_tree,-1)+tail(TPR_tree,-1)))/2
AUC_naive = sum(abs(diff(FPR_naive)) * (head(TPR_naive,-1)+tail(TPR_naive,-1)))/2
AUC_sub = sum(abs(diff(sub_model)) * (head(sub_model,-1)+tail(sub_model,-1)))/2

plot(sub_model, sub_model, col = "black", type = "l", lty = 2, main = "ROC", xlab = "False Positive Rate", ylab = "True Positive Rate")
lines(FPR_tree, TPR_tree, col = "red", type = "o")
lines(FPR_naive, TPR_naive, col = "blue", type = "o")

leg_text = c(sprintf("Random guess  AUC: %.2f", AUC_sub),sprintf("Tree model        AUC: %.2f", AUC_tree),sprintf("Naive model     AUC: %.2f", AUC_naive))

legend("bottomright", legend = leg_text, pch = c("-","o","o"), col = c("black", "red", "blue"))

```


# Assignment 3: Principal components for crime level analysis.

## Task 1

```{r}
communties <- read.csv("communities.csv")
communties <- communties[, -communties$ViolentCrimesPerPop]
#1
df <- (scale(communties))
s <- cov(df)
s.eigen <- eigen(s)
save <- c()
for (v in s.eigen$values) {
  h<- (v / sum(s.eigen$values))
  save <- c(save, h)
  }
save[1]+save[2]

nintyfive <- 0
i <- 1
while (nintyfive < 0.95) {
 nintyfive <-  save[i] + nintyfive
 i <- i + 1
}
i-1
```

The first to components explains around 42 percent of the variation. 
It needs 35 features to obtain at least 95% of variance in the data.

## Task 2

```{r}
#PRINCOMP
res <- princomp(df)
lambda <- res$sdev^2
#Plot
U <- res$loadings
x <- res$scores

plot(U[,1], main="Scoreplot, PC1")
```

The Scoreplot over PC1 shows that it seems around 15 features has a higher absolute value then 0.15 which means that they have a notable contribution to the first component. 


The 5 features that has the biggest contribution to the first component are shown in the table down. We can see that many of they features has to do with Income and family which is a very logical relationship to crime level 

```{r}
#Absolute values
features <- as.data.frame(U[,1])
features$variable <- rownames(features) 
features$`U[, 1]` <- abs(features$`U[, 1]`)
newdata <- features[order(features[,1],decreasing = TRUE ),]
newdata[1:5, ]
```
The 5 features that has the biggest contribution to the first component are shown in the table above. We can see that many of they features has to do with Income and family which is a very logical relationship to crime level 

```{r}
#pc1 AGAINST PC2

pc12 <- as.data.frame(x[,1:2])
pc12$crimes <- communties$ViolentCrimesPerPop
library(ggplot2)
p1 <- ggplot(data = pc12, aes(x=Comp.1, y=Comp.2, color=crimes))+
  geom_point() +
  theme_bw() +
  labs(title = "PC1 vs PC2"  , x = "PC1" , y = "PC2", colour = "ViolentCrimesPerPop")  +
  theme(axis.title.y = element_text(vjust = 0.5, size = 13 , face = "bold")) +
  theme(axis.title.x = element_text(vjust = 0.5 ,size = 13 , face = "bold")) +
  theme(plot.title = element_text(size = 14, face = "bold" , hjust = 0.4 )) +
  theme(axis.text.y = element_text(size = 11)) + 
  theme(axis.text.x = element_text(size = 11)) 
p1
```

The plot above shows the PC1 versus PC2 with color of Violentcrimesperpop. The picture shows that the PC1 is pretty good to separate low and high values on violentcrimesperpop. It's hard to see any pattern between PC1 and PC2.

## Task 3

```{r}
model <- lm(crimes ~ poly(Comp.1,2), data = pc12)
pc12$pred <- (model$fitted.values)

ggplot(pc12, aes(x=Comp.1, y = crimes)) +
  geom_point(color= "orange")  + theme_bw() +
  geom_line(aes(y=pred, lwd=1)) +
    labs(title = "PC1 vs Crimes"  , x = "PC1" , y = "Crimes")  +
  theme(axis.title.y = element_text(vjust = 0.5, size = 13 , face = "bold")) +
  theme(axis.title.x = element_text(vjust = 0.5 ,size = 13 , face = "bold")) +
  theme(plot.title = element_text(size = 14, face = "bold" , hjust = 0.4 )) +
  theme(axis.text.y = element_text(size = 11)) + 
  theme(axis.text.x = element_text(size = 11)) 
```

The plot above shows the violentcrimesperpop versus PC1 with the fitted values from the model above as the black line. The graph shows that the model seems to capture the connection between the target and the feature pretty good. 

## Task 4
### a) 
```{r}

library(boot)
data <- pc12
data2 <- data[order(data$Comp.1),]
mle <- lm(crimes ~ poly(Comp.1,2), data = data2)
rng <- function(data,mle2){
  n <- nrow(data)
  data1<- data.frame(crimes=data$crimes,Comp.1=data$Comp.1)
  data1$crimes <- rnorm(n,predict(mle2, newdata=data1),sd(mle2$residuals))
  return(data1)}


f1=function(data1){
  res=lm(crimes~poly(Comp.1,degree=2),data1)
  ViolentCrimesPerPopP=predict(res,newdata=data2)
  return(ViolentCrimesPerPopP)}
res2 <- boot(data2, statistic=f1, R=3000, mle = fit,ran.gen=rng  , sim="parametric")
e2 <- envelope(res2)


fit <- lm(crimes ~ poly(Comp.1,2), data = data2)
crimesP=predict(fit)
plot(data$Comp.1,data$crimes , pch=21, bg="orange", ylim=c(-0.3,1.1), main = "Confidence interval")
points(data2$Comp.1,crimesP,type="l") #plot fitted line
#plot confidence bands
points(data2$Comp.1,e2$point[2,], type="l", col="blue")
points(data2$Comp.1,e2$point[1,], type="l", col="blue")

```


The plot above shows the plot from task 3 but now with confidence intervals. You can see that the confidence intervals are wider in the beginning and the end. That is because is fewer points there compared to in the middle of the plot which gives them more uncertainty.

### b)
```{r}
f1 <- function(data1){
  res <- lm(crimes ~ poly(Comp.1,2), data = data1)
  crimesP <- predict(res, newdata = data2 )
  n <- length(data2$crimes)
  predictedP <- rnorm(n, crimesP, sd(mle$residuals))
  return(predictedP)
}


rng <- function(data, mle ){
  data1 <- data.frame(crimes=data$crimes, Comp.1=data$Comp.1)
  n <- length(data$crimes)
  data1$Crimes <- rnorm(n, predict(mle, newdata = data1), sd(mle$residuals))
  return(data1)
}



res <- boot(data2, statistic=f1, R=3000,
         mle=mle,ran.gen=rng, sim="parametric")

e <- envelope(res) #compute prediction bands
fit <- lm(crimes ~ poly(Comp.1,2), data = data2)
crimesP=predict(fit)
plot(data$Comp.1,data$crimes , pch=21, bg="orange", ylim=c(-0.3,1.1))
points(data2$Comp.1,crimesP,type="l") #plot fitted line
#plot prediction bands
points(data2$Comp.1,e$point[2,], type="l", col="blue")
points(data2$Comp.1,e$point[1,], type="l", col="blue")
```


The plot above shows the plot from task 3 but now with prediction intervals. You can see that the predictions bands have almost the same width all over the plot and that the intervals alot of the points in the plot. This is because prediction is made out of every fitted point instead of confidence interval which is made out of the fitted line. 

# Appendix:

```{r ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```
